{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install -q requests PyMuPDF pandas transformers torchvision pytorch-lightning pdf2image\n",
        "\n",
        "# Install poppler (required for pdf2image)\n",
        "!apt-get install -y poppler-utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG3FBNgEypWg",
        "outputId": "ef6896b8-b1e9-4769-d2dc-a23d420eb74b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 0s (1,944 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
        "\n",
        "# ========== Gemini API Setup ==========\n",
        "API_KEY = \"ADD YOUR API KEY\"\n",
        "BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"X-goog-api-key\": API_KEY\n",
        "}\n",
        "\n",
        "def call_gemini(prompt):\n",
        "    payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
        "    response = requests.post(BASE_URL, headers=headers, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            return response.json()['candidates'][0]['content']['parts'][0]['text']\n",
        "        except Exception:\n",
        "            print(\"âš ï¸ Could not parse Gemini response.\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"âŒ Gemini API error:\", response.text)\n",
        "        return None\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    print(\"ğŸ“„ Checking for selectable text...\")\n",
        "    text = \"\"\n",
        "    with fitz.open(file_path) as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    if text.strip():\n",
        "        print(\"âœ… Selectable text found.\")\n",
        "        return text.strip()\n",
        "    else:\n",
        "        print(\"âš ï¸ No selectable text found. Using Donut for OCR...\")\n",
        "        return extract_text_with_donut(file_path)\n",
        "\n",
        "def extract_text_with_donut(pdf_path):\n",
        "    images = convert_from_path(pdf_path, dpi=200)\n",
        "    processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    results = []\n",
        "    for img in images:\n",
        "        img = img.convert(\"RGB\")\n",
        "        pixel_values = processor(img, return_tensors=\"pt\").pixel_values.to(model.device)\n",
        "        decoder_input_ids = processor.tokenizer(\"<s>\", add_special_tokens=False, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "        outputs = model.generate(pixel_values, decoder_input_ids=decoder_input_ids, max_length=512)\n",
        "        result = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        results.append(result)\n",
        "    return \"\\n\\n\".join(results)\n",
        "\n",
        "def detect_output_format(task_detail):\n",
        "    return \"json\" if \"json\" in task_detail.lower() else \"csv\"\n",
        "\n",
        "def parse_csv_response(csv_text, expected_num_cols=None):\n",
        "    from io import StringIO\n",
        "    lines = csv_text.strip().splitlines()\n",
        "    clean_lines = []\n",
        "    for line in lines:\n",
        "        cols = [c.strip() for c in line.split(',')]\n",
        "        if expected_num_cols is None:\n",
        "            expected_num_cols = len(cols)\n",
        "        if len(cols) == expected_num_cols:\n",
        "            clean_lines.append(','.join(cols))\n",
        "        else:\n",
        "            print(f\"âš ï¸ Skipping malformed row: {line}\")\n",
        "    cleaned_csv = \"\\n\".join(clean_lines)\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(cleaned_csv))\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ Final CSV parsing failed:\", e)\n",
        "        return None\n",
        "\n",
        "def save_output(df, output_format=\"csv\", output_path=\"output\"):\n",
        "    if os.path.exists(f\"{output_path}.{output_format}\"):\n",
        "        print(f\"ğŸ“ Appending to existing {output_format.upper()} file...\")\n",
        "        if output_format == \"csv\":\n",
        "            existing_df = pd.read_csv(f\"{output_path}.csv\")\n",
        "            common_cols = [col for col in df.columns if col in existing_df.columns]\n",
        "            df = df[common_cols]\n",
        "            df.to_csv(f\"{output_path}.csv\", mode='a', header=False, index=False)\n",
        "        elif output_format == \"json\":\n",
        "            with open(f\"{output_path}.json\", \"r\") as f:\n",
        "                existing_data = json.load(f)\n",
        "            if isinstance(existing_data, list):\n",
        "                new_data = json.loads(df.to_json(orient=\"records\"))\n",
        "                existing_data.extend(new_data)\n",
        "                with open(f\"{output_path}.json\", \"w\") as f:\n",
        "                    json.dump(existing_data, f, indent=2)\n",
        "    else:\n",
        "        if output_format == \"json\":\n",
        "            df.to_json(f\"{output_path}.json\", orient=\"records\", indent=2)\n",
        "            print(f\"âœ… JSON saved to {output_path}.json\")\n",
        "        elif output_format == \"csv\":\n",
        "            df.to_csv(f\"{output_path}.csv\", index=False)\n",
        "            print(f\"âœ… CSV saved to {output_path}.csv\")\n",
        "        else:\n",
        "            print(\"âŒ Unsupported format\")\n",
        "\n",
        "def main():\n",
        "    file_path = input(\"ğŸ“ Enter the full path to your PDF file (e.g. /content/invoice.pdf): \").strip()\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"âŒ File not found at: {file_path}\")\n",
        "\n",
        "    task_detail = input(\"ğŸ“ Enter your task instruction: \").strip()\n",
        "    output_format = detect_output_format(task_detail)\n",
        "    print(f\"ğŸ“¤ Detected output format: {output_format.upper()}\")\n",
        "\n",
        "    print(\"\\nğŸ” Extracting text from PDF...\")\n",
        "    pdf_text = extract_text_from_pdf(file_path)\n",
        "\n",
        "    print(\"\\nğŸ§  Inferring expected document type from instruction...\")\n",
        "    doc_type_prompt = (\n",
        "        f\"Based on this instruction:\\n'{task_detail}'\\n\"\n",
        "        f\"What type of document is the user referring to? \"\n",
        "        f\"Reply with one word like: invoice, receipt, resume, contract, report, purchase_order, letter, etc.\"\n",
        "    )\n",
        "    expected_type = call_gemini(doc_type_prompt)\n",
        "    expected_type_clean = expected_type.strip().lower() if expected_type else None\n",
        "\n",
        "    print(f\"\\nğŸ” Validating whether the uploaded PDF is a '{expected_type_clean}' (or closely related)...\")\n",
        "    validation_prompt = (\n",
        "        f\"The user wants to work with a document of type '{expected_type_clean}'. \"\n",
        "        f\"Check the following document text and answer: \"\n",
        "        f\"Is it generally consistent with that type or a closely related business document \"\n",
        "        f\"(like invoice vs purchase order)? Say 'yes' if they are meaningfully related and not wildly different \"\n",
        "        f\"(like resume vs invoice). Reply only 'yes' or 'no'.\\n\\n{pdf_text[:3000]}\"\n",
        "    )\n",
        "    is_valid = call_gemini(validation_prompt)\n",
        "    if is_valid and is_valid.strip().lower().startswith(\"yes\"):\n",
        "        print(f\"âœ… Document matches expected type or is closely related. Continuing...\")\n",
        "    else:\n",
        "        print(f\"âŒ This file does not appear to be a '{expected_type_clean}' or closely related. No extraction performed.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nğŸ¤– Generating prompt to return plain CSV table...\")\n",
        "    csv_prompt = (\n",
        "        f\"Based on the following task: '{task_detail}', extract the relevant fields from the document \"\n",
        "        f\"and return ONLY a plain CSV table as raw text with no explanation, markdown, or code formatting.\"\n",
        "    )\n",
        "    final_prompt = csv_prompt + f\"\\n\\nHere is the document text:\\n{pdf_text}\"\n",
        "\n",
        "    print(\"\\nğŸ¤– Sending to Gemini for structured CSV output...\")\n",
        "    csv_response = call_gemini(final_prompt)\n",
        "    print(\"\\nğŸ“¦ Gemini Response:\\n\", csv_response)\n",
        "\n",
        "    df = parse_csv_response(csv_response)\n",
        "    if df is not None:\n",
        "        save_output(df, output_format)\n",
        "    else:\n",
        "        print(\"âŒ Could not parse CSV. Saving raw output.\")\n",
        "        with open(\"output_raw.txt\", \"w\") as f:\n",
        "            f.write(csv_response)\n",
        "        print(\"âœ… Saved raw Gemini output to output_raw.txt\")\n",
        "\n",
        "# Run the script\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn7nRIUyfeMG",
        "outputId": "e388c7a4-f03b-4725-b41a-30aa719a7ab6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Enter the full path to your PDF file (e.g. /content/invoice.pdf): /content/Resume.pdf\n",
            "ğŸ“ Enter your task instruction: Extract key fields like vendor name and units from this purchase order\n",
            "ğŸ“¤ Detected output format: CSV\n",
            "\n",
            "ğŸ” Extracting text from PDF...\n",
            "ğŸ“„ Checking for selectable text...\n",
            "âœ… Selectable text found.\n",
            "\n",
            "ğŸ§  Inferring expected document type from instruction...\n",
            "\n",
            "ğŸ” Validating whether the uploaded PDF is a 'purchase_order' (or closely related)...\n",
            "âŒ This file does not appear to be a 'purchase_order' or closely related. No extraction performed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4QUee3fYtxSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract item name, date, vendor, units, and total cost from this purchase order. save as spreadsheet format\n"
      ],
      "metadata": {
        "id": "jfn-9pxztBPo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}